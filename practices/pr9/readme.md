# Практическая работа №9

**Параллельные вычисления с использованием MPI**

## Цель работы

Изучить принципы параллельных вычислений с использованием **MPI (Message Passing Interface)**, освоить коллективные операции MPI и реализовать распределённые алгоритмы обработки данных. Провести анализ особенностей выполнения MPI-программ в среде Google Colab.

---

## Среда выполнения

Практическая работа выполнена и протестирована в среде:

* **Google Colab (Linux)**
* **OpenMPI**
* Компилятор: `mpic++`
* Запуск: `mpirun`

⚠️ Важно: Google Colab использует контейнеризированную среду с правами `root` и ограниченным числом доступных CPU-ядер. Это накладывает ограничения на запуск MPI-программ, которые учтены в данной работе.

---

## Структура проекта

```
task1_stats_scatterv.cpp
task2_gauss_mpi.cpp
task3_floyd_allgather.cpp
README.md
```

---

## Описание заданий

### Задание 1 — Вычисление среднего и стандартного отклонения

**Файл:** `task1_stats_scatterv.cpp`

Реализована параллельная программа для вычисления среднего значения и стандартного отклонения массива чисел.

**Используемые MPI-операции:**

* `MPI_Scatterv` — распределение массива между процессами с учётом остатка
* `MPI_Reduce` — суммирование локальных результатов

**Алгоритм:**

* Процесс с рангом 0 создаёт массив случайных чисел
* Массив распределяется между процессами
* Каждый процесс вычисляет локальную сумму и сумму квадратов
* Результаты собираются на процессе 0

---

### Задание 2 — Метод Гаусса

**Файл:** `task2_gauss_mpi.cpp`

Реализована упрощённая распределённая версия метода Гаусса для решения системы линейных уравнений.

**Используемые MPI-операции:**

* `MPI_Scatter` — распределение строк матрицы
* `MPI_Bcast` — рассылка ведущей строки
* `MPI_Gather` — сбор результатов

**Особенности:**

* Используется диагонально-доминирующая матрица
* Реализация носит учебный характер
* Частичный выбор главного элемента (pivoting) не используется

---

### Задание 3 — Алгоритм Флойда–Уоршелла

**Файл:** `task3_floyd_allgather.cpp`

Реализован параллельный алгоритм поиска кратчайших путей между всеми парами вершин графа.

**Используемые MPI-операции:**

* `MPI_Scatter` — распределение строк матрицы расстояний
* `MPI_Allgather` — синхронизация матрицы на каждом шаге алгоритма

**Алгоритм:**

* Матрица расстояний распределяется между процессами
* На каждой итерации `k` выполняется обновление локальных строк
* Обновлённые данные синхронизируются между всеми процессами

---

## Особенности запуска в Google Colab

### Запуск MPI от имени root

В Google Colab все программы запускаются от имени пользователя `root`.
Поэтому для запуска MPI используется флаг:

```bash
--allow-run-as-root
```

Пример:

```bash
mpirun --allow-run-as-root -np 1 ./task1
```

---

### Ограничение на количество процессов (slots)

Среда Google Colab часто предоставляет **один виртуальный CPU-ядро**, поэтому при запуске с `-np > 1` может возникать ошибка нехватки ресурсов.

Для обхода этого ограничения используется режим **oversubscribe**:

```bash
mpirun --allow-run-as-root --oversubscribe -np 4 ./task1
```

⚠️ В этом режиме несколько MPI-процессов работают на одном CPU-ядре.
Это не даёт ускорения, но позволяет корректно протестировать MPI-логику.

---

## Компиляция и запуск

### Компиляция:

```bash
mpic++ -O3 -std=c++17 task1_stats_scatterv.cpp -o task1
mpic++ -O3 -std=c++17 task2_gauss_mpi.cpp -o task2
mpic++ -O3 -std=c++17 task3_floyd_allgather.cpp -o task3
```

### Запуск (Colab):

```bash
mpirun --allow-run-as-root -np 1 ./task1
mpirun --allow-run-as-root -np 1 ./task2
mpirun --allow-run-as-root -np 1 ./task3
```

или с oversubscribe:

```bash
mpirun --allow-run-as-root --oversubscribe -np 4 ./task1
```

---

## Выводы

В ходе работы были изучены основные принципы параллельных вычислений с использованием MPI, реализованы распределённые алгоритмы обработки данных и исследованы особенности выполнения MPI-программ в среде Google Colab. Несмотря на ограничения среды, корректность работы алгоритмов была подтверждена.

---

### Примечание

Ограничения Google Colab связаны с особенностями виртуализированной среды и не являются ошибками в реализации MPI-программ. В полноценной Linux-системе или вычислительном кластере данные программы масштабируются корректно.

