# Практическая работа №10  
Анализ и профилирование параллельных и распределённых программ

## Цель работы
Изучить методы анализа производительности параллельных программ на CPU, GPU,
гибридных CPU+GPU приложений и распределённых MPI-программ. Освоить базовые
подходы к профилированию и оценке масштабируемости.

---

## Среда выполнения

Практическая работа выполнена в среде:

- Google Colab (Linux)
- C++ / OpenMP
- CUDA
- MPI (OpenMPI)

Особенности среды Google Colab:
- программы запускаются от имени root;
- ограниченное количество доступных CPU-ядер;
- MPI используется в режиме oversubscribe или с одним процессом.

---


---

## Описание заданий

### Задание 1 — Анализ производительности OpenMP

Файл: `task1_openmp_performance.cpp`

- Реализована параллельная программа с OpenMP.
- Выполнено измерение времени выполнения с помощью `omp_get_wtime()`.
- Определена доля последовательной и параллельной части.
- Проанализировано влияние числа потоков в контексте закона Амдала.

---

### Задание 2 — Доступ к памяти на GPU (CUDA)

Файл: `task2_gpu_memory_access.cu`

- Реализованы два CUDA-ядра:
  - с коалесцированным доступом к памяти;
  - с некоалесцированным доступом.
- Время выполнения измерено с использованием `cudaEvent`.
- Показано влияние паттерна доступа к памяти на производительность GPU.

---

### Задание 3 — Гибридное приложение CPU + GPU

Файл: `task3_hybrid_async.cu`

- Реализован гибридный алгоритм обработки массива.
- Использованы асинхронные передачи данных (`cudaMemcpyAsync`).
- Применены CUDA streams для перекрытия вычислений и передачи данных.
- Снижены накладные расходы взаимодействия CPU и GPU.

---

### Задание 4 — Анализ масштабируемости MPI-программы

Файл: `task4_mpi_scaling.cpp`

- Реализована MPI-программа для вычисления суммы массива.
- Использована коллективная операция `MPI_Reduce`.
- Выполнена оценка strong scaling и weak scaling.
- Проанализировано влияние коммуникаций на масштабируемость.

---

## Запуск в Google Colab

### OpenMP:
```bash
g++ -O3 -fopenmp task1_openmp_performance.cpp -o task1
./task1
```
Cuda:
```bash
nvcc -O3 task2_gpu_memory_access.cu -o task2
./task2
```
Hybrid CPU + GPU:
```bash
nvcc -O3 task3_hybrid_async.cu -o task3
./task3
```
MPI:
```bash
mpic++ -O3 task4_mpi_scaling.cpp -o task4
mpirun --allow-run-as-root -np 1 ./task4
```

### Выводы

В ходе работы были изучены основные методы анализа производительности
параллельных и распределённых программ. Эксперименты показали, что
ускорение вычислений ограничивается последовательной частью кода,
накладными расходами синхронизации и коммуникаций, а также доступом к памяти.